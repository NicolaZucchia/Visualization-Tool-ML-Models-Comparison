{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv(\"diabetes_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Outcome'])\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting\n",
    "model1 = xgboost.XGBRegressor(random_state=42).fit(X,y)\n",
    "\n",
    "# NN\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "model2.compile(loss='mse', optimizer='adam')\n",
    "model2.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting results\n",
    "\n",
    "preds1 = model1.predict(X)\n",
    "preds2 = model2.predict(X)\n",
    "\n",
    "explainer1 = shap.Explainer(model1)\n",
    "explainer2 = shap.DeepExplainer(model2, X.values)\n",
    "\n",
    "shap_values1 = explainer1(X)\n",
    "shap_values2 = explainer2.shap_values(X.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2Models_Diabetes.txt\", \"w\") as f:\n",
    "    for i in range(0, len(y)):\n",
    "        f.write(str(preds1[i]))\n",
    "        f.write(\", \")\n",
    "        f.write(str(preds2[i]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y = max(y)\n",
    "for i in range(0, len(y)):\n",
    "    preds1[i] = preds1[i] / max_y\n",
    "    preds2[i] = preds2[i] / max_y\n",
    "\n",
    "with open(\"2Models_Diabetes_preds_shaps.txt\", \"w\") as f:\n",
    "    for i in range(0, len(y)):\n",
    "        f.write(str(preds1[i]))\n",
    "        f.write(\", \")\n",
    "        f.write(str(preds2[i]))\n",
    "        f.write(\", \")\n",
    "        tmp_lst1 = []\n",
    "        tmp_lst2 = []\n",
    "        for j in range(0, len(shap_values1[i].values)):\n",
    "            tmp_lst1.append(str(shap_values1[i].values[j]))\n",
    "            tmp_lst1.append(\", \")\n",
    "        tmp_strng1 = ''.join(tmp_lst1)\n",
    "        f.write(tmp_strng1)\n",
    "        tmp_lst2 = []\n",
    "        for j in range(0, len(shap_values2[0][i])):\n",
    "            tmp_lst2.append(str(shap_values2[0][i][j]))\n",
    "            tmp_lst2.append(\", \")\n",
    "        tmp_strng2 = ''.join(tmp_lst2)\n",
    "        nw_ts2 = tmp_strng2[:-2]\n",
    "        f.write(nw_ts2)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def random_rows(input_file, output_file, num_rows):\n",
    "    with open(input_file, 'r') as f_in:\n",
    "        with open(output_file, 'w') as f_out:\n",
    "            lines = f_in.readlines()\n",
    "            num_lines = len(lines)\n",
    "            if num_lines < num_rows:\n",
    "                raise ValueError(f\"Not enough lines in input file. Only found {num_lines} lines.\")\n",
    "            chosen_indices = set(random.sample(range(num_lines), num_rows))\n",
    "            for i, line in enumerate(lines):\n",
    "                if i in chosen_indices:\n",
    "                    f_out.write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_diffs = []\n",
    "for i in range(0, len(y)):\n",
    "    cur_shaps = []\n",
    "    for j in range(0,len(shap_values1[i].values)):\n",
    "        cur_d = shap_values1[i].values[j] - shap_values2[0][i][j]\n",
    "        cur_shaps.append(cur_d)\n",
    "    shap_diffs.append(cur_shaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(shap_diffs)\n",
    "feature_names = list(X.columns)\n",
    "df.columns = feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and modules\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Extract the features into X\n",
    "X1 = df.iloc[:, 1:].values\n",
    "\n",
    "# Calculate the silhouette score for different numbers of clusters\n",
    "scores = []\n",
    "for n_clusters in range(2, 20):\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X1)\n",
    "    silhouette_avg = silhouette_score(X1, cluster_labels)\n",
    "    scores.append(silhouette_avg)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "optimal_n_clusters = np.argmax(scores) + 2\n",
    "print(\"Optimal number of clusters =\", optimal_n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_n_clusters)\n",
    "kmeans.fit(df)\n",
    "labels=kmeans.labels_\n",
    "df['cluster'] = labels\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE and scatterplot on original dataset, coloring it basing on cluster beloning\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df2 = pd.DataFrame(X_tsne, columns=['tsne1','tsne2'])\n",
    "df2['label'] = y\n",
    "df2['cluster'] = df['cluster']\n",
    "\n",
    "sns.scatterplot(data=df2, x='tsne1', y='tsne2', hue='cluster', palette='bright')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df2['cluster'].value_counts()\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
