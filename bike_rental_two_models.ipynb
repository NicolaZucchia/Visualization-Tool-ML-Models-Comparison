{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "import shap\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\Nicol\\\\Desktop\\\\bike_rental.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['cnt'])\n",
    "y = df['cnt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 30892.7969\n",
      "Epoch 2/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 28918.2461\n",
      "Epoch 3/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 27546.4590\n",
      "Epoch 4/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 26276.6191\n",
      "Epoch 5/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 25426.0332\n",
      "Epoch 6/50\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 24865.4590\n",
      "Epoch 7/50\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 24795.1621\n",
      "Epoch 8/50\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 24574.5859\n",
      "Epoch 9/50\n",
      "435/435 [==============================] - 1s 3ms/step - loss: 24308.7773\n",
      "Epoch 10/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 24297.5176\n",
      "Epoch 11/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 24100.0059\n",
      "Epoch 12/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 24077.9453\n",
      "Epoch 13/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 23838.6816\n",
      "Epoch 14/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 23812.9375\n",
      "Epoch 15/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23566.6738\n",
      "Epoch 16/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23545.5527\n",
      "Epoch 17/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23426.8555\n",
      "Epoch 18/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23364.4023\n",
      "Epoch 19/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23376.2402\n",
      "Epoch 20/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 23010.2051\n",
      "Epoch 21/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22895.7578\n",
      "Epoch 22/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22956.5195\n",
      "Epoch 23/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22853.5449\n",
      "Epoch 24/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22738.1875\n",
      "Epoch 25/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22654.6191\n",
      "Epoch 26/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22539.7930\n",
      "Epoch 27/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 22526.9082\n",
      "Epoch 28/50\n",
      "435/435 [==============================] - 0s 1ms/step - loss: 22291.4531\n",
      "Epoch 29/50\n",
      "435/435 [==============================] - 0s 1ms/step - loss: 22281.3008\n",
      "Epoch 30/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 22210.7324\n",
      "Epoch 31/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 22127.5938\n",
      "Epoch 32/50\n",
      "435/435 [==============================] - 1s 1ms/step - loss: 22082.2422\n",
      "Epoch 33/50\n",
      "435/435 [==============================] - 0s 1ms/step - loss: 22031.5938\n",
      "Epoch 34/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21874.3125\n",
      "Epoch 35/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21881.0977\n",
      "Epoch 36/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21804.9980\n",
      "Epoch 37/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21924.2969\n",
      "Epoch 38/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21719.4219\n",
      "Epoch 39/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21611.6895\n",
      "Epoch 40/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21406.0801\n",
      "Epoch 41/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21544.3828\n",
      "Epoch 42/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21426.8906\n",
      "Epoch 43/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21423.5664\n",
      "Epoch 44/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21334.1816\n",
      "Epoch 45/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21341.9629\n",
      "Epoch 46/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21164.0469\n",
      "Epoch 47/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21440.1465\n",
      "Epoch 48/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21265.1133\n",
      "Epoch 49/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21124.0215\n",
      "Epoch 50/50\n",
      "435/435 [==============================] - 1s 2ms/step - loss: 21190.0645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24d25ec7688>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting\n",
    "model1 = xgboost.XGBRegressor(random_state=42).fit(X,y)\n",
    "\n",
    "# NN\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "model2.add(Dense(1))\n",
    "model2.compile(loss='mse', optimizer='adam')\n",
    "model2.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have provided over 5k background samples! For better performance consider using smaller random sample.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    }
   ],
   "source": [
    "# Predicting results\n",
    "\n",
    "pred1 = model1.predict(X)\n",
    "preds2 = model2.predict(X)\n",
    "\n",
    "explainer1 = shap.Explainer(model1)\n",
    "explainer2 = shap.DeepExplainer(model2, X.values)\n",
    "\n",
    "shap_values1 = explainer1(X)\n",
    "shap_values2 = explainer2.shap_values(X.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"XGB_vs_NN.txt\", \"w\") as f:\n",
    "    for i in range(0, len(y)):\n",
    "        f.write(str(pred1[i]))\n",
    "        f.write(\", \")\n",
    "        f.write(str(preds2[i]))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y = max(y)\n",
    "for i in range(0, len(y)):\n",
    "    pred1[i] = pred1[i] / max_y\n",
    "    preds2[i] = preds2[i] / max_y\n",
    "\n",
    "with open(\"two_models_preds_shaps.txt\", \"w\") as f:\n",
    "    for i in range(0, len(y)):\n",
    "        f.write(str(pred1[i]))\n",
    "        f.write(\", \")\n",
    "        f.write(str(preds2[i]))\n",
    "        f.write(\", \")\n",
    "        tmp_lst1 = []\n",
    "        tmp_lst2 = []\n",
    "        for j in range(0, len(shap_values1[i].values)):\n",
    "            tmp_lst1.append(str(shap_values1[i].values[j]))\n",
    "            tmp_lst1.append(\", \")\n",
    "        tmp_strng1 = ''.join(tmp_lst1)\n",
    "        f.write(tmp_strng1)\n",
    "        tmp_lst2 = []\n",
    "        for j in range(0, len(shap_values2[0][i])):\n",
    "            tmp_lst2.append(str(shap_values2[0][i][j]))\n",
    "            tmp_lst2.append(\", \")\n",
    "        tmp_strng2 = ''.join(tmp_lst2)\n",
    "        nw_ts2 = tmp_strng2[:-2]\n",
    "        f.write(nw_ts2)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(X)\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_diffs = []\n",
    "for i in range(0, len(y)):\n",
    "    cur_shaps = []\n",
    "    for j in range(0,len(shap_values1[i].values)):\n",
    "        cur_d = shap_values1[i].values[j] - shap_values2[0][i][j]\n",
    "        cur_shaps.append(cur_d)\n",
    "    shap_diffs.append(cur_shaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(shap_diffs)\n",
    "feature_names = list(X.columns)\n",
    "df.columns = feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and modules\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Extract the features into X\n",
    "X1 = df.iloc[:, 1:].values\n",
    "\n",
    "# Calculate the silhouette score for different numbers of clusters\n",
    "scores = []\n",
    "for n_clusters in range(2, 20):\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = clusterer.fit_predict(X1)\n",
    "    silhouette_avg = silhouette_score(X1, cluster_labels)\n",
    "    scores.append(silhouette_avg)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "optimal_n_clusters = np.argmax(scores) + 2\n",
    "print(\"Optimal number of clusters =\", optimal_n_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=optimal_n_clusters)\n",
    "kmeans.fit(df)\n",
    "labels=kmeans.labels_\n",
    "df['cluster'] = labels\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE and scatterplot on original dataset, coloring it basing on cluster beloning\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "df2 = pd.DataFrame(X_tsne, columns=['tsne1','tsne2'])\n",
    "df2['label'] = y\n",
    "df2['cluster'] = df['cluster']\n",
    "\n",
    "sns.scatterplot(data=df2, x='tsne1', y='tsne2', hue='cluster', palette='bright')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df2['cluster'].value_counts()\n",
    "print(counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
